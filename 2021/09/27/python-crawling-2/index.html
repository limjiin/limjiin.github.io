<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"limjiin.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="지난 시간에는 간단하게 웹크롤링 기초를 익혔지만,이번에는 lxml, css 등 다양한 웹크롤링을 배워보자!">
<meta property="og:type" content="article">
<meta property="og:title" content="python-crawler 2편">
<meta property="og:url" content="https://limjiin.github.io/2021/09/27/python-crawling-2/index.html">
<meta property="og:site_name" content="야무진의 기술 블로그">
<meta property="og:description" content="지난 시간에는 간단하게 웹크롤링 기초를 익혔지만,이번에는 lxml, css 등 다양한 웹크롤링을 배워보자!">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-09-27T09:17:03.753Z">
<meta property="article:modified_time" content="2021-10-11T04:52:26.818Z">
<meta property="article:author" content="limjiin">
<meta property="article:tag" content="python">
<meta property="article:tag" content="crawler">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://limjiin.github.io/2021/09/27/python-crawling-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>python-crawler 2편 | 야무진의 기술 블로그</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">야무진의 기술 블로그</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">데이터 마케터 짜그리이지만, 계속 해보겠습니다.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://limjiin.github.io/2021/09/27/python-crawling-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="limjiin">
      <meta itemprop="description" content="All stories about git, sql, python">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="야무진의 기술 블로그">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          python-crawler 2편
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-27 18:17:03" itemprop="dateCreated datePublished" datetime="2021-09-27T18:17:03+09:00">2021-09-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-10-11 13:52:26" itemprop="dateModified" datetime="2021-10-11T13:52:26+09:00">2021-10-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2021/09/27/python-crawling-2/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2021/09/27/python-crawling-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>지난 시간에는 간단하게 웹크롤링 기초를 익혔지만,<br>이번에는 lxml, css 등 다양한 웹크롤링을 배워보자!</p>
<span id="more"></span>

<h1 id="Section01-1"><a href="#Section01-1" class="headerlink" title="Section01-1"></a>Section01-1</h1><ul>
<li>파이썬 크롤링 기초</li>
<li>urllib 사용법 및 기본 스크랩핑</li>
</ul>
<pre>
<code>
import urllib.request as req

# 파일 Url
img_url = 'https://file.mk.co.kr/meet/neds/2021/06/image_readtop_2021_535745_16226846584668330.jpg'
html_url = 'https://www.google.com/'

# 다운 받을 경로
save_path1 = 'C:/test1.jpg'
save_path2 = 'C:/index.html'

# 예외 처리
try:
    file1, header1 = req.urlretrieve(img_url, save_path1)
    file2, header2 = req.urlretrieve(html_url, save_path2)

except Exception as e:
    print('Download failed')
    print(e)

else: 
    # Header 정보 출력
    print(header1)
    print(header2)
    
    # 다운로드 파일 정보
    print(f'Filename1 &#123;file1&#125;')
    print(f'Filename2 &#123;file2&#125;')
    
    # 성공
    print('Download Succeed')
</code>
</pre>

<h1 id="Section01-2"><a href="#Section01-2" class="headerlink" title="Section01-2"></a>Section01-2</h1><ul>
<li>파이썬 크롤링 기초</li>
<li>urlopen 함수 기초 사용법</li>
<li>urllib.request 예외 처리</li>
</ul>
<pre>
<code>
import urllib.request as req
from urllib.error import URLError, HTTPError

# 다운로드 경로 및 파일명
path_list = ['C:/test1.jpg', 'C:/index.html']

# 다운로드 리소스 url
target_url = ["http://post.phinf.naver.net/20160621_169/1466482468068lmSHj_JPEG/If7GeIbOPZuYwI-GI3xU7ENRrlfI.jpg",
              "http://google.com"]

for i, url in enumerate(target_url):
    # 예외 처리
    try:
        # 웹 수신 정보 읽기
        response = req.urlopen(url)
        
        # 수신 내용
        contents = response.read()
        print('-----------------------------------')
        
        # 상태 정보 중간 출력
        print(f'Header Info-&#123;i&#125; : &#123;response.info()&#125;')
        print(f'HTTP Status Code : &#123;response.getcode()&#125;')
        print()
        print('-----------------------------------')
        
        # 파일 쓰기
        with open(path_list[i], 'wb') as c:
            c.write(contents)
        
    except HTTPError as e:
        print('Download failed')
        print('HTTPError Code : ', e.code)
    except URLError as e:
        print('Download failed')
        print('URL Error Reason : ', e.reason)
            
    # 성공
    else:
        print()
        print('Download Succeed.')
</code>
</pre>

<h1 id="Section01-3"><a href="#Section01-3" class="headerlink" title="Section01-3"></a>Section01-3</h1><ul>
<li>파이썬 크롤링 기초</li>
<li>lxml 사용 기초 스크랩핑(1)</li>
</ul>
<h4 id="lxml-설치하기"><a href="#lxml-설치하기" class="headerlink" title="lxml 설치하기"></a>lxml 설치하기</h4><pre>
<code>
!pip install lxml

!pip install cssselect

# 설치팩 확인하기
pip list
</code>
</pre>

<h4 id="lxml로-스크랩핑하기"><a href="#lxml로-스크랩핑하기" class="headerlink" title="lxml로 스크랩핑하기"></a>lxml로 스크랩핑하기</h4><pre>
<code>
import lxml.html
import requests

def main():
    '''
    네이버 메인 뉴스 스탠드 스크랩핑 메인 함수
    '''

    # 스크랩핑 대상 URL
    response = requests.get('https://www.naver.com/') # Get, Post

    # 신문사 링크 리스트 획득
    urls = scrape_news_list_page(response)

    # 결과 출력
    for url in urls:
        print(url)
        # 파일 쓰기
        # 생략

def scrape_news_list_page(response):
    # url 리스트 선언
    urls = []

    # 태그 정보 문자열 저장
    root = lxml.html.fromstring(response.content)

    for a in root.cssselect('.api_list .api_item a.api_link'):
        # 링크
        url = a.get('herf')
        urls.append(url)
    return urls


# 스크랩핑 시작
if __name__ == '__main__':
    main()
</code>
</pre>

<h1 id="Section01-4"><a href="#Section01-4" class="headerlink" title="Section01-4"></a>Section01-4</h1><ul>
<li>파이썬 크롤링 기초</li>
<li>lxml 사용 기초 스크랩핑(2)</li>
</ul>
<pre>
<code>
import requests
from lxml.html import fromstring, tostring


def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인 함수
    """
    # 세션 사용
    session = requests.Session()

    # 스크랩핑 대상 URL
    response = session.get('http://www.naver.com/')
    # 신문사 정보 딕셔너리 획득
    urls = scrape_news_list_page(response)

    # 딕셔너리 확인
    # print(urls)

    # 결과 출력
    for name, url in urls.items():
        print(name, url)


def scrape_news_list_page(response):
    # URL 딕셔너리 선언
    urls = &#123;&#125;
    # 태그 정보 문자열 저장
    root = fromstring(response.content)

    # 문서내 경로 절대 경로 변환
    root.make_links_absolute(response.url)

    for a in root.xpath('//ul[@class="api_list"]/li[@class="api_item"]/a[@class="api_link"]'):
        # a 구조 확인
        # print(dir(a))

        # a 문자열 출력
        # print(tostring(a, pretty_print=True))

        # 신문사, 링크 추출 함수
        name, url = extract_contents(a)

        # 딕셔너리 삽입
        urls[name] = url
    return urls


def extract_contents(dom):
    # 링크 주소
    link = dom.get('href')
    # dom 구조 확인
    # print(tostring(dom, pretty_print=True))

    # 신문사 명
    name = dom.xpath('./img')[0].get('alt')  # xpath('./img')
    return name, link


#  스크랩핑 시작
if __name__ == '__main__':
    main()
</code>
</pre>

<h1 id="Section02-1"><a href="#Section02-1" class="headerlink" title="Section02-1"></a>Section02-1</h1><ul>
<li>기본 스크랩핑 실습</li>
<li>Get 방식 데이터 통신(1)</li>
</ul>
<pre>
<code>
import urllib.request
from urllib.parse import urlparse

# 기본 요청 1 (encar)
url = 'http://www.encar.com'

mem = urllib.request.urlopen(url)

# 여러 정보
print(f'type : &#123;type(mem)&#125;')
print(f'geturl : &#123;mem.geturl()&#125;')
print(f'status : &#123;mem.status&#125;')
print(f'headers : &#123;mem.getheaders()&#125;')
print(f'getcode : &#123;mem.getcode()&#125;')
print("read : &#123;&#125;".format(mem.read(1).decode('utf-8'))) # 바이트 수
print('parse : &#123;&#125;'.format(urlparse('http://www.encar.co.kr?test=test')))
print('parse : &#123;&#125;'.format(urlparse('http://www.encar.co.kr?test=test').query))

# 기본 요청 2 (ipify)
API = 'https://api.ipify.org'

# Get 방식 Parameter
values = &#123;
    'format' : 'json'
&#125;

print(f'before param : &#123;values&#125;')
params = urllib.parse.urlencode(values)
print(f'after param : &#123;params&#125;')

# 요청 URL 생성
URL = API + '?' + params
print(f'요청 URL : &#123;URL&#125;')

# 수신 데이터 읽기
data = urllib.request.urlopen(URL).read()

# 수신 데이터 디코딩
text = data.decode('utf-8')
print(f'reponse : &#123;text&#125;')
</code>
</pre>

<p><a target="_blank" rel="noopener" href="https://www.ipify.org/">https://www.ipify.org/</a></p>
<h1 id="Section02-2"><a href="#Section02-2" class="headerlink" title="Section02-2"></a>Section02-2</h1><ul>
<li>기본 스크랩핑 실습</li>
<li>Get 방식 데이터 통신(2) - RSS</li>
</ul>
<pre>
<code>
import urllib.request
import urllib.parse

# 행정 안전부 : https://www.mois.go.kr
# 행정 안전부 RSS API URL
API = 'https://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp'

params = []

for num in [1001, 1012, 1013, 1014]:
    params.append(dict(ctxCd=num))

# 중간 확인
print(params)

# 연속해서 4회 요청
for c in params:
    # 파라미터 출력
    # print(c)
    # URL 인코딩
    param = urllib.parse.urlencode(c)
    
    # URL 완성
    url = API + '?' + param
    
    # URL 출력
    print('url : ', url)
    
    # 요청
    res_data = urllib.request.urlopen(url).read()
    # print(res_data)
    
    # 수신 후 디코딩
    contents = res_data.decode('UTF-8')
    
    # 출력
    print('*' * 100)
    print(contents)
</code>
</pre>

<h1 id="Section02-3"><a href="#Section02-3" class="headerlink" title="Section02-3"></a>Section02-3</h1><ul>
<li>기본 스크랩핑 실습</li>
<li>다음 주식 정보 가져오기</li>
</ul>
<pre>
<code>
!pip install fake-useragent

import json
import urllib.request as req
from fake_useragent import UserAgent

# Fake Header 정보 (가상으로 User-agent 생성)
ua = UserAgent()
print(ua.ie)
print(ua.msie)
print(ua.chrome)
print(ua.safari)
print(ua.random)

# 헤더 정보
headers = &#123;
    'User-agent' : ua.ie,
    'referer' : 'https://finance.daum.net/'    
&#125;

# 다음 주식 요청 URL
url = 'https://finance.daum.net/api/search/ranks?limit=10'

# 요청 
res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')

# 응답 데이터 확인(Json Data)
print('res', res)

# 응답 데이터 str -> json 변환 및 data 값 출력
rank_json = json.loads(res)['data']

# 중간 확인
print('중간 확인 : ', rank_json, '\n')

for elm in rank_json:
    print(type(elm))
    print('순위 : &#123;&#125;, 금액 : &#123;&#125;, 회사명 : &#123;&#125;'.format(elm['rank'], elm.get('tradePrice'), elm['name']))
    pass
</code>
</pre>

<h1 id="Section03-1"><a href="#Section03-1" class="headerlink" title="Section03-1"></a>Section03-1</h1><ul>
<li>Requests</li>
<li>requests 사용 스크랩핑(1) - Session</li>
</ul>
<pre>
<code>
import requests
# 세션 활성화
s = requests.Session()
r = s.get('https://www.naver.com/')

# 수신 데이터
# print(r.text)

# 수신 상태 코드
# print(f'Status Code : &#123;r.status_code&#125;')

# 확인
# print(f'Ok? : &#123;r.ok&#125;')

s = requests.Session()

# 쿠키 return
r1 = s.get('https://httpbin.org/cookies', cookies=&#123;'name': 'kim1'&#125;)
print(r1.text)

# 쿠키 set
r2 = s.get('https://httpbin.org/cookies/set', cookies=&#123;'name': 'kim2'&#125;)
print(r2.text)

# User-Agent
url = 'https://httpbin.org'
headers = &#123;'user-agent' : 'nice-man_1.0.0_win10_ram16_home_chrome'&#125;

# Header 정보 전송
r3 = s.get(url, headers=headers)
print(r3.text)

# 세션 비활성화
s.close()

# WIth문 사용(권장) -> 파일, DB, HTTP
with requests.Session() as s:
    r = s.get('https://www.daum.net/')
    print(r.text)
    print(r.ok)
</code>
</pre>

<h1 id="Section03-2"><a href="#Section03-2" class="headerlink" title="Section03-2"></a>Section03-2</h1><ul>
<li>Requests</li>
<li>requests 사용 스크랩핑(2) - JSON</li>
</ul>
<pre>
<code>
import json
import requests

s = requests.Session()

# 100개 JSON 데이터 요청
r = s.get('https://httpbin.org/stream/100', stream=True)

# 수신 확인
print(r.text)

# Encoding 확인
print(f'Encoding : &#123;r.encoding&#125;')

# if r.encoding is None:
#     r.encoding = 'utf-8'
# print(f'After Encoding : &#123;r.encoding&#125;')

for line in r.iter_lines(decode_unicode=True):
    # 라인 출력 후 타입 확인
    # print(line)
    # print(type(line))

    # JSON(Dict) 변환 후 타입 확인
    b = json.loads(line) # str -> dict
    # print(b)
    # print(type(b))
    # 정보 내용 출력
    for k, v in b.items():
        print(f'key : &#123;k&#125;, Value : &#123;v&#125;')

    print()
    print()

s.close()

r = s.get('https://jsonplaceholder.typicode.com/todos/1')

# Header 정보
print(r.headers)

# 본문 정보
print(r.text)

# json 변환
print(r.json())

# key 변환
print(r.json().keys())

# 값 변환
print(r.json().values())

# 인코딩 변환
print(r.encoding)

# 바이너리 정보
print(r.content)

s.close()

s = requests.Session()

# 100개 JSON 데이터 요청
r = s.get('https://jsonplaceholder.typicode.com/posts')

# 수신 확인
print(r.text)
</code>
</pre>

<h1 id="Section03-3"><a href="#Section03-3" class="headerlink" title="Section03-3"></a>Section03-3</h1><ul>
<li>Requests</li>
<li>requests 사용 스크랩핑(3) - Rest API</li>
</ul>
<h6 id="Rest-API-GET-POST-DELETE-PUT-UPDATE-REPLACE-FETCH-UPDATE-MODIFY"><a href="#Rest-API-GET-POST-DELETE-PUT-UPDATE-REPLACE-FETCH-UPDATE-MODIFY" class="headerlink" title="Rest API : GET, POST, DELETE, PUT: UPDATE, REPLACE(FETCH : UPDATE, MODIFY)"></a>Rest API : GET, POST, DELETE, PUT: UPDATE, REPLACE(FETCH : UPDATE, MODIFY)</h6><ul>
<li>중요 : URL을 활용해서 자원의 상태 정보를 주고 받는 모든 것</li>
<li>GET : <a target="_blank" rel="noopener" href="http://www.movies.com/movies">www.movies.com/movies</a> : 영화를 전부 조회</li>
<li>GET : <a target="_blank" rel="noopener" href="http://www.movies.com/movies/:id">www.movies.com/movies/:id</a> : 아이디인 영화를 조회</li>
<li>POST : <a target="_blank" rel="noopener" href="http://www.movies.com/movies/">www.movies.com/movies/</a> : 영화를 생성</li>
<li>PUT : <a target="_blank" rel="noopener" href="http://www.movies.com/movies/">www.movies.com/movies/</a> : 영화를 수정</li>
<li>DELETE : <a target="_blank" rel="noopener" href="http://www.movies.com/movies/">www.movies.com/movies/</a> : 영화를 삭제</li>
</ul>
<pre>
<code>
import requests

# 세션 활성화
s = requests.Session()

# 예제1
r = s.get('https://api.github.com/events')

# 수신상태 체크
r.raise_for_status()

# 출력
print(r.text)

# 예제2
# 쿠키 설정
jar = requests.cookies.RequestsCookieJar()

# 쿠키 삽입
jar.set('name', 'niceman', domain='httpbin.org', path='/cookies')

# 요청
r = s.get('https://httpbin.org/cookies', cookies=jar)

# 출력
print(r.text)

# 예제3
r = s.get('https://github.com', timeout=5)

# 출력
print(r.text)

# 예제4
r = s.post('https://httpbin.org/post', data=&#123;'id':'test77', 'pw':'111'&#125;, cookies=jar)

# 출력
print(r.text)
print(r.headers)

# 예제5
# 요청(POST)
payload1 = &#123;'id':'test77', 'pw':'111'&#125;
payload2 = (('id', 'test77'), ('pw', '111'))

r = s.post('https://httpbin.org/post', data=payload2)

# 출력
print(r.text)

# 예제6(PUT)
r = s.put('https://httpbin.org/put', data=payload1)

# 출력
print(r.text)

# 예제7(DELETE)
r = s.delete('https://httpbin.org/delete', data=&#123;'id':1&#125;)

# 출력
print(r.text)

# 예제
r = s.delete('https://jsonplaceholder.typicode.com/posts/1')
print(r.ok)
print(r.text)
print(r.headers)

s.close()
</code>
</pre>

<h1 id="Section04-1"><a href="#Section04-1" class="headerlink" title="Section04-1"></a>Section04-1</h1><ul>
<li>BeautifulSoup</li>
<li>BeautifulSoup 사용 스크랩핑(1) - 기본 사용법</li>
</ul>
<pre>
<code>
!pip install beautifulsoup4

from bs4 import BeautifulSoup

    html = '''
    <html>
    <head>
        <title>The Dormouse's story</title>
    </head>
    <body>
        <h1>this is h1 area</h1>
        <h2>this is h2 area</h2>
        <p class='title'><b>The Dormouse's story</b></p>
        <p class='story'><b>Once upon a time there were three little sisters.
            <a target="_blank" rel="noopener" href='https://example.com/elsie' class='sister' id='link1'>Elsie</a>
            <a target="_blank" rel="noopener" href='https://example.com/lacie' class='sister' id='link1'>Lacie</a>
            <a data-text='test' data-io='link3' target="_blank" rel="noopener" href='https://example.com/little' class='sister' id='link3'>Title</a>
        </p>
        <p class='story'>
            story...
        </p>
    </body>
    </html>
    '''
</code>
</pre>

<h4 id="예제로-BeautifulSoup-익히기"><a href="#예제로-BeautifulSoup-익히기" class="headerlink" title="예제로 BeautifulSoup 익히기"></a>예제로 BeautifulSoup 익히기</h4><ul>
<li>예제1(BeautifulSoup 기초)</li>
</ul>
<pre>
<code>
# bs4 초기화
soup = BeautifulSoup(html, 'html.parser')

# 타입 확인
print('soup', type(soup))
print('prettify', soup.prettify())

# h1 태그 접근
h1 = soup.html.body.h1
print('h1', h1)

# p 태그 접근
p1 = soup.html.body.p
print('p1', p1)

# 다음 태그
p2 = p1.next_sibling.next_sibling
print('p2', p2)

# 텍스트 출력1
print('h1 >>', h1.string)

# 텍스트 출력2
print('p1 >>', p1.string)

# 함수 확인
print(dir(p2))

# 다음 엘리먼트 확인
# print(list(p2.next_element))

# 반복 출력 확인
for v in p2.next_element:
    pass
    # print(v)
</code>
</pre>

<ul>
<li>예제2(Find, Find_all) </li>
</ul>
<pre>
<code>
# bs4
soup2 = BeautifulSoup(html, 'html.parser')

# a 태그 모두 선택(조건에 맞는 것을 모두 가져옴)
link1 = soup2.find_all('a') # limit=2 옵션
# print(type(link1))

# 리스트 요소 확인
print('links', link1)

# 중요
link2 = soup2.find_all('a', class_='sister') # id='link2', string='title', string=['Elsie']
print(link2)

for t in link2:
    print(t)

# 처음 발견한 a 태그 선택
link3 = soup.find('a')

print()
print(link3)
print(link3.string)
print(link3.text)

# 다중 조건(중요)
link4 = soup.find('a', &#123;'class': 'sister', 'data-io': 'link3'&#125;)

print()
print(link4)
print(link4.text)
print(link4.string)

# css 선택자 : select, select_one
# 태그로 접근 : find, find_all
# 예제3(select, select_one)

link5 = soup.select_one('p.title > b')
print()
print(link5)
print(link5.text)
print(link5.string)

link6 = soup.select_one('a#link1')
print()
print(link6)
print(link6.text)
print(link6.string)

link7 = soup.select_one("a[data-text='test']")
print()
print(link7)
print(link7.text)
print(link7.string) # . : 클래스, # : id

# 선택자에 맞는 전체 선택
link8 = soup.select('p.story > a')
print()
print(link8) # 텍스트가 3개라서 오류 발생 for문으로 출력해야 함
# print(link8.string)

link9 = soup.select('p.story > a:nth-of-type(2)')
print()
print(link9)

link10 = soup.select('p.story')
print()
print(link10)

for t in link10:
    temp = t.find_all('a')
    print(temp)
    
    if temp:
        for v in temp:
            print('>>>>>', v)
            print('>>>>>', v.string)
        else:
            print('-----', t)
            print('-----', t.string)
</code>
</pre>

<h1 id="Section04-2"><a href="#Section04-2" class="headerlink" title="Section04-2"></a>Section04-2</h1><ul>
<li>BeautifulSoup</li>
<li>BeautifulSoup 사용 스크랩핑(2) - 이미지 다운로드</li>
</ul>
<pre>
<code>
import os
import urllib.parse as rep
import urllib.request as req
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

# Header 정보 초기화
opener = req.build_opener()

# User-Agent 정보
opener.addheaders = [('User-agent', UserAgent().ie)]

# Header 정보 삽입
req.install_opener(opener)

# 네이버 이미지 기본 URL(크롬 개발자 도구)
base = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='

# 검색어
quote = rep.quote_plus('호랑이')

# URL 완성
url = base + quote

# 요청 URL 확인
print(f'Request URL : &#123;url&#125;')

# Request
res = req.urlopen(url)

# 이미지 저장 경로
savePath = 'C:/imagedown' # C:\\imagedown\\

# 폴더 생성 예외 처리(문제 발생 시 프로그램 종료)
try:
    # 기본 폴더가 있는지 체크
    if not (os.path.isdir(savePath)):
        # 없으면 폴더 생성
        os.makedirs(os.path.join(savePath))
except OSError as e:
    # 에러 내용
    print('folder creation failed.')
    print(f'folder name : &#123;e.filename&#125;')

    # 런타임 에러
    raise RuntiomeError('System Exit!')
else:
    # 폴더 생성이 되었거나, 존재할 경우
    print('folder is created!')

# bs4 초기화
soup = BeautifulSoup(res, 'html.parser')

# print(soup.prettify())

# select 사용
img_list = soup.select('div.img_area > a.thumb._thumb > img')

# find_all 사용할 경우
img_list2 = soup.find_all('a', class_='thumb _thumb')

for v in img_list2:
    img_t = v.find('img')
    print(img_t.attrs['data-source'])

# print(img_list)

for i img in enumerate(img_list, 1):
    print()
    print()
    # 속성 확인
    # print(img['data-source'], i)
    
    # 저장 파일명 및 경로
    fullFileName = os.path.join(savePath, savePath + str(i) + 'png')
    
    # 파일명
    print(fullFileName)
    
    # 다운로드 요청(URL, 다운로드 경로)
    # 이미지 최대 50장 저장
    req.urlretrieve(img['data-source'], fulFileName)
    
# 다운로드 완료 시 출력
print('downloads succeed!')
__
</code>
</pre>

<h1 id="Section04-3"><a href="#Section04-3" class="headerlink" title="Section04-3"></a>Section04-3</h1><ul>
<li>BeautifulSoup</li>
<li>BeautifulSoup 사용 스크랩핑(3) - 로그인 처리</li>
</ul>
<pre>
<code>
import requests as req
from fake_useragent import UserAgent
from bs4 import BeautifulSoup

login_info = &#123;
    'redirectUrl' : 'https://www.danawa.com/',
    'loginMemberType' : 'general',
    'id' : '1234',
    'password' : 'a1234'
&#125;

# Headers 정보
headers = &#123;
    'User-Agent' : UserAgent().chrome,
    'Referer' : 'https://auth.danawa.com/login?url=http%3A%2F%2Fwww.danawa.com%2F'
&#125;

with req.session() as s:
    # Request(로그인 시도)
    res = s.post('https://auth.danawa.com/login', login_info, headers=headers)
    
    # 로그인 시도 실패 시 예외
    if res.status_code != 200:
        raise Exception('Login failed!')
        
    # 본문 수신 데이터 확인
    # print(res.content.decode('UTF-8'))
    
    # 로그인 성공 후 세션 정보를 가지고 페이지 이동
    res = s.get('https://buyer.danawa.com/order/Order/orderList', headers=headers)
    
    # Euc-kr(한글 깨질 경우)
    # res.encoding = 'euc-kr'
    
    # 페이지 이동 후 수신 데이터 확인
    print(res.text)

        # bs4 초기화
    soup = BeautifulSoup(res.text, 'html.parser')

    # 로그인 성공 체크
    check_name = soup.find('p', class_='user')
    print(check_name)

    if check_name is None:
        raise Exception('Login failed, worong Password')

    # 선택자 사용
    info_list = soup.select('div.my_info > div.sub_info > ul.info_list > li')

    # 확인
    print(info_list)

    # 이 부분에서 재요청, 파일 다운로드, DB 저장, 파일쓰기(엑셀)

    # 제목
    print()
    print('***** My Info *****')

    for v in info_list:
        # 속성 메소드 확인
        # print(dir(v))

        # 필요한 텍스트 추출
        proc, val = v.find('span').string.strip(), v.find('strong').string.strip()
        print(f'&#123;proc&#125; : &#123;val&#125;')
</code>
</pre>

<h1 id="Section05-1"><a href="#Section05-1" class="headerlink" title="Section05-1"></a>Section05-1</h1><ul>
<li>Selenium</li>
<li>Selenium 사용 실습(1) - 설정 및 기본 테스트</li>
</ul>
<pre>
<code>
!pip install selenium

# selenium 임포트
from selenium import webdriver

# webdriver 설정(Chrome, Firefox 등)
browser = webdriver.Chrome('./chromedriver.exe')

# 크롬 브라우저 내부 대기
browser.implicitly_wait(5)

# 속성 확인
print(dir(browser))

# 브라우저 사이즈
# browser.set_window_size(1920, 1280) # maximize_window(), minimize_window()
browser.maximize_window()

# 페이지 이동
browser.get('https://www.daum.net')

# 페이지 내용
print(f'Page Contents : &#123;browser.page_source&#125;')

# 세션 값 출력
print(f'Session ID : &#123;browser.session_id&#125;')

# 타이틀 출력
print(f'Title : &#123;browser.title&#125;')

# 현재 URL 출력
print(f'URL : &#123;browser.current_url&#125;')

# 현재 쿠키 정보 출력
print(f'URL : &#123;browser.get_cookies&#125;')

# 검색창 input 선택
element = browser.find_element_by_css_selector('div.inner_search > input.tf_keyword')

# 검색어 입력
element.send_keys('방탄소년단')

# 검색(From Submit)
element.submit()

# 스크린 샷 저장 1
browser.save_screenshot('C:/website_ch1.jpg')

# 스크린 샷 저장 2
browser.get_screenshot_as_file('C:/website_ch2.jpg')
</code>
</pre>

<h1 id="Section05-2"><a href="#Section05-2" class="headerlink" title="Section05-2"></a>Section05-2</h1><ul>
<li>Selenium</li>
<li>Selenium 사용 실습(2) - 실습 프로젝트(1)</li>
</ul>
<pre>
<code>
!pip install xlsxwriter

chrome_options = Options()
chrome_options.add_argument("--headless")

# 엑셀 처리 선언
workbook = xlsxwriter.Workbook("C:/crawling_result.xlsx")

# 워크 시트
worksheet = workbook.add_worksheet()

# webdriver 설정(Chrome, Firefox 등) - 일반 모드
browser = webdriver.Chrome('./chromedriver.exe')

# 크롬 브라우저 내부 대기
browser.implicitly_wait(5)

# 브라우저 사이즈
browser.set_window_size(1920, 1280)  # maximize_window(), minimize_window()

# 페이지 이동
browser.get('http://prod.danawa.com/list/?cate=112758&15main_11_02')

# 3초간 대기
time.sleep(3)

# 현재 페이지
cur_page_num = 1

# 크롤링 페이지 수
target_crawl_num = 3

# 엑셀 행 수
ins_cnt = 1

while cur_page_num <= target_crawl_num:

    # bs4 초기화
    soup = BeautifulSoup(browser.page_source, "html.parser")

    # 소스코드 정리
    # print(soup.prettify())

    # 메인 상품 리스트 선택
    pro_list = soup.select('div.main_prodlist.main_prodlist_list > ul > li')

    # 상품 리스트 확인
    # print(pro_list)

    # 페이지 번호 출력
    print('****** Current Page : &#123;&#125;'.format(cur_page_num), ' ******')
    print()

    # 필요 정보 추출(페이지 이동 크롤링)
    for v in pro_list:
        # 임시 출력
        # print(v)

        # 불필요한 영역 패스
        if not v.find('div', class_='ad_header'):
            # 상품명, 이미지, 가격
            prod_name = v.select('p.prod_name > a')[0].text.strip()
            prod_price = v.select('p.price_sect > a')[0].text.strip()

            # 이미지 요청 후 바이트 변환
            # img_data = BytesIO(req.urlopen(v.select('a.thumb_link > img')[0]).read())

            # 엑셀 저장(텍스트)
            worksheet.write('A%s' % ins_cnt, prod_name)
            worksheet.write('B%s' % ins_cnt, prod_price)

            # 엑셀 저장(이미지)
            # worksheet.insert_image('C%s' % ins_cnt, prod_name, &#123;'image_data': img_data&#125;)

            # 다음 행 증가
            ins_cnt += 1

        print()

    print()

    # 페이지 별 스크린 샷 저장
    browser.save_screenshot("c:/target_page&#123;&#125;.png".format(cur_page_num))

    # 페이지 증가
    cur_page_num += 1

    if cur_page_num > target_crawl_num:
        print('Crawling Succeed.')
        break

    # 페이지 이동 클릭
    WebDriverWait(browser, 2).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.number_wrap > a:nth-child(&#123;&#125;)'.format(cur_page_num)))).click()

    # 소스코드 리로드
    # browser.refresh()

    # BeautifulSoup 인스턴스 삭제
    del soup

    # 4초간 대기
    time.sleep(4)

# 브라우저 종료
browser.quit()

# 엑셀 파일 닫기
workbook.close()
</code>
</pre>

<h6 id="엑셀-이미지-저장-제외한-크롤링"><a href="#엑셀-이미지-저장-제외한-크롤링" class="headerlink" title="엑셀, 이미지 저장 제외한 크롤링"></a>엑셀, 이미지 저장 제외한 크롤링</h6><pre>
<code>
# selenium 임포트
from selenium import webdriver
import time
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import urllib.request as req

chrome_options = Options()
chrome_options.add_argument('--headless')

# 엑셀 처리 선언
workbook = xlsxwriter.Workbook('C:/crawling_result.xlsx')

# 워크 시트
worksheet = workbook.add_worksheet()

# webdriver 설정(Chrome, Firefox 등) - Headless 모드(실행 안됨)
# browser = webdriver.Chrome('./chromedriver.exe', options=chrome_options)

# webdriver 설정(Chrome, Firefox 등) - 일반 모드
browser = webdriver.Chrome('./chromedriver.exe')

# 크롬 브라우저 내부 대기
browser.implicitly_wait(5)

# 브라우저 사이즈
browser.set_window_size(1920, 1280) # maximize_window(), minimize_window()

# 페이지 이동
browser.get('http://prod.danawa.com/list/?cate=112758')

# 2초간 대기
time.sleep(2)

# 현재 페이지
cur_page = 1

# 크롤링 페이지 수
target_crawl_num = 5

while cur_page <= target_crawl_num:

    #bs4 초기화
    soup = BeautifulSoup(browser.page_source, 'html.parser')

    # 소스코드 정리
    # print(soup.prettify())

    # 메인 상품 리소스 선택
    pro_list = soup.select('div.main_prodlist.main_prodlist_list > ul > li')

    # 상품 리스트 확인
    # print(pro_list)

    # 페이지 번호 출력
    print(f'********** Current Page : &#123;cur_page&#125;', '**********')
    print()

    # 필요 정보 추출
    for v in pro_list:
        # 임시 출력
        # print(v)

        if not v.find('div', class_='ad_header'):

            # 상품명, 이미지, 가격
            print()
            print(v.select('p.prod_name > a')[0].text.strip())
            print(v.select('a.thumb_link > img'))
            print(v.select('p.price_sect > a')[0].text.strip())


        print()
    print()

    # 페이지 별 스크린 샷 저장
    browser.save_screenshot(f'C:/target_page&#123;cur_page&#125;.png')

    # 페이지 증가
    cur_page += 1

    if cur_page > target_crawl_num:
        print('Crawling Succeed.')
        break

    # 페이지 이동 클릭
    WebDriverWait(browser, 2).until(EC.presence_of_element_located((By.CSS_SELECTOR, f'div.number_wrap > a:nth-child(&#123;cur_page&#125;)'))).click()

    # 3초간 대기
    time.sleep(3)

    # BeautifulSoup 인스턴스 삭제
    del soup

# 브라우저 종료
browser.close()
</code>
</pre>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/crawler/" rel="tag"># crawler</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/09/17/python-crawling/" rel="prev" title="python-crawler 1편">
      <i class="fa fa-chevron-left"></i> python-crawler 1편
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/08/sql-basic-1/" rel="next" title="sql-basic 1편">
      sql-basic 1편 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Section01-1"><span class="nav-number">1.</span> <span class="nav-text">Section01-1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section01-2"><span class="nav-number">2.</span> <span class="nav-text">Section01-2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section01-3"><span class="nav-number">3.</span> <span class="nav-text">Section01-3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#lxml-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">lxml 설치하기</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lxml%EB%A1%9C-%EC%8A%A4%ED%81%AC%EB%9E%A9%ED%95%91%ED%95%98%EA%B8%B0"><span class="nav-number">3.0.0.2.</span> <span class="nav-text">lxml로 스크랩핑하기</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section01-4"><span class="nav-number">4.</span> <span class="nav-text">Section01-4</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section02-1"><span class="nav-number">5.</span> <span class="nav-text">Section02-1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section02-2"><span class="nav-number">6.</span> <span class="nav-text">Section02-2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section02-3"><span class="nav-number">7.</span> <span class="nav-text">Section02-3</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section03-1"><span class="nav-number">8.</span> <span class="nav-text">Section03-1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section03-2"><span class="nav-number">9.</span> <span class="nav-text">Section03-2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section03-3"><span class="nav-number">10.</span> <span class="nav-text">Section03-3</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Rest-API-GET-POST-DELETE-PUT-UPDATE-REPLACE-FETCH-UPDATE-MODIFY"><span class="nav-number">10.0.0.0.0.1.</span> <span class="nav-text">Rest API : GET, POST, DELETE, PUT: UPDATE, REPLACE(FETCH : UPDATE, MODIFY)</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section04-1"><span class="nav-number">11.</span> <span class="nav-text">Section04-1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number">12.</span> <span class="nav-text">this is h1 area</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">12.1.</span> <span class="nav-text">this is h2 area</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EC%98%88%EC%A0%9C%EB%A1%9C-BeautifulSoup-%EC%9D%B5%ED%9E%88%EA%B8%B0"><span class="nav-number">12.1.0.1.</span> <span class="nav-text">예제로 BeautifulSoup 익히기</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section04-2"><span class="nav-number">13.</span> <span class="nav-text">Section04-2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section04-3"><span class="nav-number">14.</span> <span class="nav-text">Section04-3</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section05-1"><span class="nav-number">15.</span> <span class="nav-text">Section05-1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Section05-2"><span class="nav-number">16.</span> <span class="nav-text">Section05-2</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%EC%97%91%EC%85%80-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%A0%80%EC%9E%A5-%EC%A0%9C%EC%99%B8%ED%95%9C-%ED%81%AC%EB%A1%A4%EB%A7%81"><span class="nav-number">16.0.0.0.0.1.</span> <span class="nav-text">엑셀, 이미지 저장 제외한 크롤링</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">limjiin</p>
  <div class="site-description" itemprop="description">All stories about git, sql, python</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">33</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/limjiin" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;limjiin" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:limjiin0413@gmail.com" title="E-Mail → mailto:limjiin0413@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.naver.com/lljin0413" title="Blog → https:&#x2F;&#x2F;blog.naver.com&#x2F;lljin0413" rel="noopener" target="_blank"><i class="fab fa-blog fa-fw"></i>Blog</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/ji_in_l" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;ji_in_l" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">limjiin</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://jin.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://limjiin.github.io/2021/09/27/python-crawling-2/";
    this.page.identifier = "2021/09/27/python-crawling-2/";
    this.page.title = "python-crawler 2편";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://jin.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
